# Machine Learning - Algoritmos Principales

> **GuÃ­a de los algoritmos mÃ¡s importantes con explicaciones conceptuales**
> Ãšltima actualizaciÃ³n: Enero 2026

---

## Tabla de Contenidos

1. [[#Algoritmos de RegresiÃ³n|RegresiÃ³n]]
2. [[#Algoritmos de ClasificaciÃ³n|ClasificaciÃ³n]]
3. [[#Ãrboles de DecisiÃ³n|Ãrboles de DecisiÃ³n]]
4. [[#Ensemble Methods|MÃ©todos Ensemble]]
5. [[#Support Vector Machines|SVM]]
6. [[#K-Nearest Neighbors|KNN]]

---

## Algoritmos de RegresiÃ³n

### Linear Regression (RegresiÃ³n Lineal)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CONCEPTO: REGRESIÃ“N LINEAL                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  El algoritmo mÃ¡s simple: encuentra una lÃ­nea recta que         â”‚
â”‚  mejor se ajusta a los datos.                                   â”‚
â”‚                                                                 â”‚
â”‚  ECUACIÃ“N:  y = Î²â‚€ + Î²â‚xâ‚ + Î²â‚‚xâ‚‚ + ... + Î²â‚™xâ‚™                  â”‚
â”‚                                                                 â”‚
â”‚  â€¢ y = valor predicho                                           â”‚
â”‚  â€¢ Î²â‚€ = intercepto (valor cuando todas las x son 0)             â”‚
â”‚  â€¢ Î²áµ¢ = coeficientes (cuÃ¡nto afecta cada feature)               â”‚
â”‚  â€¢ xáµ¢ = features (caracterÃ­sticas)                              â”‚
â”‚                                                                 â”‚
â”‚  VISUALIZACIÃ“N (1 feature):                                     â”‚
â”‚                                                                 â”‚
â”‚    y â”‚         â€¢    /                                           â”‚
â”‚      â”‚       â€¢   /                                              â”‚
â”‚      â”‚     â€¢  /   â† LÃ­nea que minimiza errores                  â”‚
â”‚      â”‚   â€¢ /                                                    â”‚
â”‚      â”‚ â€¢ /                                                      â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ x                                            â”‚
â”‚                                                                 â”‚
â”‚  Â¿CÃ“MO ENCUENTRA LA MEJOR LÃNEA?                                â”‚
â”‚  Minimiza la suma de errores al cuadrado (OLS):                 â”‚
â”‚  min Î£(y_real - y_pred)Â²                                        â”‚
â”‚                                                                 â”‚
â”‚  SUPUESTOS (para que funcione bien):                            â”‚
â”‚  1. RelaciÃ³n lineal entre X e y                                 â”‚
â”‚  2. Errores normalmente distribuidos                            â”‚
â”‚  3. No multicolinealidad (features no correlacionadas)          â”‚
â”‚  4. Homocedasticidad (varianza constante de errores)            â”‚
â”‚                                                                 â”‚
â”‚  CUÃNDO USAR:                                                   â”‚
â”‚  âœ… RelaciÃ³n aproximadamente lineal                             â”‚
â”‚  âœ… Necesitas interpretabilidad                                 â”‚
â”‚  âœ… Baseline rÃ¡pido                                             â”‚
â”‚  âŒ Relaciones complejas/no lineales                            â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```python
# ========================================
# REGRESIÃ“N LINEAL COMPLETA
# ========================================
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Datos de ejemplo: Predecir salario por aÃ±os de experiencia
np.random.seed(42)
experiencia = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)
salario = np.array([30, 35, 40, 48, 55, 60, 68, 75, 82, 90]) + np.random.randn(10) * 3

# Dividir datos
X_train, X_test, y_train, y_test = train_test_split(
    experiencia, salario, test_size=0.2, random_state=42
)

# Entrenar modelo
model = LinearRegression()
model.fit(X_train, y_train)

# Coeficientes (interpretables)
print(f"Intercepto (Î²â‚€): {model.intercept_:.2f}")
print(f"Coeficiente (Î²â‚): {model.coef_[0]:.2f}")
print(f"\nInterpretaciÃ³n: Por cada aÃ±o de experiencia,")
print(f"el salario aumenta ${model.coef_[0]:.2f}k")

# Predecir
y_pred = model.predict(X_test)
print(f"\nRÂ² Score: {r2_score(y_test, y_pred):.3f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.2f}k")

# Visualizar
plt.figure(figsize=(10, 6))
plt.scatter(experiencia, salario, color='blue', label='Datos reales')
plt.plot(experiencia, model.predict(experiencia), color='red', label='PredicciÃ³n')
plt.xlabel('AÃ±os de Experiencia')
plt.ylabel('Salario (miles)')
plt.title('RegresiÃ³n Lineal: Salario vs Experiencia')
plt.legend()
plt.grid(True)
plt.show()
```

### Polynomial Regression

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CONCEPTO: REGRESIÃ“N POLINOMIAL                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  ExtensiÃ³n de lineal para capturar relaciones curvas.           â”‚
â”‚  AÃ±ade potencias de las features como nuevas features.          â”‚
â”‚                                                                 â”‚
â”‚  Lineal:     y = Î²â‚€ + Î²â‚x                                       â”‚
â”‚  Grado 2:    y = Î²â‚€ + Î²â‚x + Î²â‚‚xÂ²                                â”‚
â”‚  Grado 3:    y = Î²â‚€ + Î²â‚x + Î²â‚‚xÂ² + Î²â‚ƒxÂ³                         â”‚
â”‚                                                                 â”‚
â”‚  VISUALIZACIÃ“N:                                                 â”‚
â”‚                                                                 â”‚
â”‚  Lineal         Grado 2          Grado 3                        â”‚
â”‚    /              âˆ©                âˆ¿                            â”‚
â”‚   /              / \              /\                            â”‚
â”‚  /              /   \            /  \                           â”‚
â”‚                                     \/                          â”‚
â”‚                                                                 â”‚
â”‚  âš ï¸ CUIDADO: Grado muy alto = Overfitting                       â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline

# Datos con relaciÃ³n cuadrÃ¡tica
X = np.array([1, 2, 3, 4, 5, 6, 7, 8]).reshape(-1, 1)
y = np.array([1, 4, 9, 16, 25, 36, 49, 64]) + np.random.randn(8) * 2

# Pipeline: transformar + modelo
poly_model = Pipeline([
    ('poly', PolynomialFeatures(degree=2)),  # AÃ±ade xÂ²
    ('linear', LinearRegression())
])

poly_model.fit(X, y)
y_pred = poly_model.predict(X)

print(f"RÂ²: {r2_score(y, y_pred):.3f}")

# Comparar diferentes grados
plt.figure(figsize=(12, 4))
for i, degree in enumerate([1, 2, 5], 1):
    plt.subplot(1, 3, i)
    model = Pipeline([
        ('poly', PolynomialFeatures(degree=degree)),
        ('linear', LinearRegression())
    ])
    model.fit(X, y)
    
    X_plot = np.linspace(0, 9, 100).reshape(-1, 1)
    plt.scatter(X, y, color='blue')
    plt.plot(X_plot, model.predict(X_plot), color='red')
    plt.title(f'Grado {degree}')
plt.tight_layout()
plt.show()
```

### Ridge y Lasso (RegularizaciÃ³n)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           CONCEPTO: REGULARIZACIÃ“N L1 y L2                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  AÃ±aden una "penalizaciÃ³n" para evitar coeficientes muy grandes â”‚
â”‚  y prevenir overfitting.                                        â”‚
â”‚                                                                 â”‚
â”‚  RIDGE (L2): Penaliza suma de coeficientes al cuadrado          â”‚
â”‚  Loss = MSE + Î± Ã— Î£Î²Â²                                           â”‚
â”‚  â†’ Reduce coeficientes pero no los elimina                      â”‚
â”‚  â†’ Bueno cuando todas las features son Ãºtiles                   â”‚
â”‚                                                                 â”‚
â”‚  LASSO (L1): Penaliza suma de valores absolutos                 â”‚
â”‚  Loss = MSE + Î± Ã— Î£|Î²|                                          â”‚
â”‚  â†’ Puede hacer coeficientes EXACTAMENTE 0                       â”‚
â”‚  â†’ SelecciÃ³n automÃ¡tica de features                             â”‚
â”‚  â†’ Bueno para eliminar features irrelevantes                    â”‚
â”‚                                                                 â”‚
â”‚  ELASTIC NET: CombinaciÃ³n de ambos                              â”‚
â”‚  Loss = MSE + Î±â‚ Ã— Î£|Î²| + Î±â‚‚ Ã— Î£Î²Â²                              â”‚
â”‚                                                                 â”‚
â”‚  Î± (alpha) = Fuerza de regularizaciÃ³n                           â”‚
â”‚  â€¢ Î± = 0: Sin regularizaciÃ³n (igual que Linear)                 â”‚
â”‚  â€¢ Î± grande: Mucha regularizaciÃ³n (coeficientes â†’ 0)            â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```python
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.preprocessing import StandardScaler

# Datos con muchas features (algunas irrelevantes)
np.random.seed(42)
n_samples = 100
X = np.random.randn(n_samples, 10)  # 10 features
# Solo las primeras 3 features son relevantes
y = 3*X[:, 0] + 2*X[:, 1] - X[:, 2] + np.random.randn(n_samples) * 0.5

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Escalar es importante para regularizaciÃ³n
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Comparar modelos
models = {
    'Linear': LinearRegression(),
    'Ridge (L2)': Ridge(alpha=1.0),
    'Lasso (L1)': Lasso(alpha=0.1),
    'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5)
}

print("ComparaciÃ³n de coeficientes:\n")
print(f"{'Modelo':<15} {'RÂ²':<8} Coeficientes")
print("-" * 70)

for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    score = model.score(X_test_scaled, y_test)
    coefs = model.coef_
    print(f"{name:<15} {score:.3f}   {np.round(coefs, 2)}")

# Lasso hace algunos coeficientes = 0 (features 4-10 son irrelevantes)
```

---

## Algoritmos de ClasificaciÃ³n

### Logistic Regression

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CONCEPTO: REGRESIÃ“N LOGÃSTICA                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  A pesar del nombre, es para CLASIFICACIÃ“N, no regresiÃ³n.       â”‚
â”‚  Predice la PROBABILIDAD de pertenecer a una clase.             â”‚
â”‚                                                                 â”‚
â”‚  FUNCIÃ“N SIGMOIDE:                                              â”‚
â”‚                                                                 â”‚
â”‚     P(y=1) = 1 / (1 + e^(-z))    donde z = Î²â‚€ + Î²â‚xâ‚ + ...     â”‚
â”‚                                                                 â”‚
â”‚       1 â”¤         ___________                                   â”‚
â”‚         â”‚        /                                              â”‚
â”‚     0.5 â”¤â”€â”€â”€â”€â”€â”€â”€/â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â† Umbral de decisiÃ³n             â”‚
â”‚         â”‚      /                                                â”‚
â”‚       0 â”¤_____/                                                 â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                  â”‚
â”‚              -4  -2   0   2   4                                 â”‚
â”‚                                                                 â”‚
â”‚  Si P(y=1) > 0.5 â†’ Predice clase 1                              â”‚
â”‚  Si P(y=1) â‰¤ 0.5 â†’ Predice clase 0                              â”‚
â”‚                                                                 â”‚
â”‚  VENTAJAS:                                                      â”‚
â”‚  âœ… Probabilidades interpretables                               â”‚
â”‚  âœ… Coeficientes interpretables                                 â”‚
â”‚  âœ… RÃ¡pido de entrenar                                          â”‚
â”‚  âœ… Funciona bien con features linealmente separables           â”‚
â”‚                                                                 â”‚
â”‚  LIMITACIONES:                                                  â”‚
â”‚  âŒ Asume relaciÃ³n lineal entre features y log-odds             â”‚
â”‚  âŒ No captura relaciones complejas                             â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```python
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification
from sklearn.metrics import classification_report, roc_auc_score

# Crear datos de clasificaciÃ³n binaria
X, y = make_classification(
    n_samples=1000, n_features=10, n_informative=5,
    n_redundant=2, random_state=42
)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Entrenar
model = LogisticRegression(random_state=42)
model.fit(X_train, y_train)

# Predecir clases
y_pred = model.predict(X_test)

# Predecir PROBABILIDADES (muy Ãºtil)
y_prob = model.predict_proba(X_test)[:, 1]

print("Reporte de ClasificaciÃ³n:")
print(classification_report(y_test, y_pred))
print(f"AUC-ROC: {roc_auc_score(y_test, y_prob):.3f}")

# Interpretar coeficientes
print("\nImportancia de features (coeficientes):")
for i, coef in enumerate(model.coef_[0]):
    print(f"  Feature {i}: {coef:+.3f}")
```

### Naive Bayes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CONCEPTO: NAIVE BAYES                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Basado en el Teorema de Bayes con la suposiciÃ³n "ingenua"      â”‚
â”‚  de que todas las features son independientes.                  â”‚
â”‚                                                                 â”‚
â”‚  TEOREMA DE BAYES:                                              â”‚
â”‚                                                                 â”‚
â”‚  P(clase|features) = P(features|clase) Ã— P(clase)               â”‚
â”‚                      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”‚
â”‚                            P(features)                          â”‚
â”‚                                                                 â”‚
â”‚  "Â¿CuÃ¡l es la probabilidad de que sea spam dado que             â”‚
â”‚   contiene 'gratis' y 'dinero'?"                                â”‚
â”‚                                                                 â”‚
â”‚  TIPOS:                                                         â”‚
â”‚  â€¢ GaussianNB: Features continuas (distribuciÃ³n normal)         â”‚
â”‚  â€¢ MultinomialNB: Features discretas (conteo de palabras)       â”‚
â”‚  â€¢ BernoulliNB: Features binarias (0/1)                         â”‚
â”‚                                                                 â”‚
â”‚  VENTAJAS:                                                      â”‚
â”‚  âœ… MUY rÃ¡pido (ideal para textos)                              â”‚
â”‚  âœ… Funciona bien con pocos datos                               â”‚
â”‚  âœ… Escala bien a muchas features                               â”‚
â”‚  âœ… Probabilidades calibradas                                   â”‚
â”‚                                                                 â”‚
â”‚  LIMITACIONES:                                                  â”‚
â”‚  âŒ Asume independencia (raramente verdad)                      â”‚
â”‚  âŒ No captura interacciones entre features                     â”‚
â”‚                                                                 â”‚
â”‚  CASO DE USO IDEAL: ClasificaciÃ³n de texto (spam, sentimiento)  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```python
from sklearn.naive_bayes import GaussianNB, MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer

# ========================================
# EJEMPLO 1: ClasificaciÃ³n de texto (spam)
# ========================================
textos = [
    "oferta gratis dinero ahora",
    "reuniÃ³n maÃ±ana 10am",
    "gana premio millonario",
    "informe proyecto adjunto",
    "descuento especial urgente",
    "confirmaciÃ³n cita mÃ©dico"
]
labels = [1, 0, 1, 0, 1, 0]  # 1=spam, 0=no spam

# Vectorizar texto (convertir a nÃºmeros)
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(textos)

# Entrenar Naive Bayes Multinomial (para conteos)
nb = MultinomialNB()
nb.fit(X, labels)

# Predecir nuevo email
nuevo = ["dinero gratis sin esfuerzo"]
nuevo_vec = vectorizer.transform(nuevo)
prob = nb.predict_proba(nuevo_vec)[0]
print(f"P(no spam): {prob[0]:.2%}")
print(f"P(spam): {prob[1]:.2%}")

# ========================================
# EJEMPLO 2: Features numÃ©ricas
# ========================================
from sklearn.datasets import load_iris

iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(
    iris.data, iris.target, test_size=0.2
)

gnb = GaussianNB()
gnb.fit(X_train, y_train)
print(f"\nIris Accuracy: {gnb.score(X_test, y_test):.3f}")
```

---

## Ãrboles de DecisiÃ³n

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CONCEPTO: ÃRBOLES DE DECISIÃ“N                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Aprende reglas de decisiÃ³n en forma de Ã¡rbol.                  â”‚
â”‚  Como un juego de "20 preguntas".                               â”‚
â”‚                                                                 â”‚
â”‚  ESTRUCTURA:                                                    â”‚
â”‚                                                                 â”‚
â”‚             [Â¿Edad > 30?]           â† Nodo raÃ­z                 â”‚
â”‚              /        \                                         â”‚
â”‚           SÃ­          No                                        â”‚
â”‚           /            \                                        â”‚
â”‚   [Â¿Ingreso > 50k?]    [Â¿Estudiante?]  â† Nodos internos         â”‚
â”‚      /      \            /      \                               â”‚
â”‚    SÃ­       No         SÃ­       No                              â”‚
â”‚    /         \         /          \                             â”‚
â”‚  [Compra]  [No]    [Compra]    [No]    â† Hojas (decisiÃ³n)       â”‚
â”‚                                                                 â”‚
â”‚  Â¿CÃ“MO DECIDE QUÃ‰ PREGUNTA HACER?                               â”‚
â”‚  Busca la divisiÃ³n que mejor separa las clases usando:          â”‚
â”‚  â€¢ Gini Impurity: Mide "mezcla" de clases (0=puro)              â”‚
â”‚  â€¢ Entropy/Information Gain: Mide desorden                      â”‚
â”‚                                                                 â”‚
â”‚  VENTAJAS:                                                      â”‚
â”‚  âœ… MUY interpretable (puedes ver las reglas)                   â”‚
â”‚  âœ… No requiere escalado de datos                               â”‚
â”‚  âœ… Maneja features categÃ³ricas y numÃ©ricas                     â”‚
â”‚  âœ… Captura relaciones no lineales                              â”‚
â”‚                                                                 â”‚
â”‚  LIMITACIONES:                                                  â”‚
â”‚  âŒ Propenso a overfitting                                      â”‚
â”‚  âŒ Inestable (pequeÃ±os cambios â†’ Ã¡rbol diferente)              â”‚
â”‚  âŒ Sesgado hacia features con muchos niveles                   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```python
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt

# Cargar datos
iris = load_iris()
X, y = iris.data, iris.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Entrenar Ã¡rbol (con lÃ­mites para evitar overfitting)
tree = DecisionTreeClassifier(
    max_depth=3,           # Profundidad mÃ¡xima
    min_samples_split=10,  # MÃ­nimo para dividir
    min_samples_leaf=5,    # MÃ­nimo en hojas
    random_state=42
)
tree.fit(X_train, y_train)

print(f"Accuracy: {tree.score(X_test, y_test):.3f}")

# Visualizar el Ã¡rbol
plt.figure(figsize=(20, 10))
plot_tree(
    tree,
    feature_names=iris.feature_names,
    class_names=iris.target_names,
    filled=True,
    rounded=True,
    fontsize=10
)
plt.title("Ãrbol de DecisiÃ³n - Iris")
plt.tight_layout()
plt.show()

# Ver importancia de features
print("\nImportancia de features:")
for name, imp in zip(iris.feature_names, tree.feature_importances_):
    print(f"  {name}: {imp:.3f}")
```

---

## Ensemble Methods

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CONCEPTO: MÃ‰TODOS ENSEMBLE                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  "La sabidurÃ­a de las multitudes"                               â”‚
â”‚  Combinar mÃºltiples modelos dÃ©biles â†’ modelo fuerte             â”‚
â”‚                                                                 â”‚
â”‚  DOS ESTRATEGIAS PRINCIPALES:                                   â”‚
â”‚                                                                 â”‚
â”‚  1. BAGGING (Bootstrap Aggregating)                             â”‚
â”‚     â€¢ Entrena modelos en PARALELO                               â”‚
â”‚     â€¢ Cada modelo ve una muestra diferente (con reemplazo)      â”‚
â”‚     â€¢ Combina por votaciÃ³n (clasificaciÃ³n) o promedio (reg)     â”‚
â”‚     â€¢ Reduce VARIANZA (overfitting)                             â”‚
â”‚     Ejemplo: Random Forest                                      â”‚
â”‚                                                                 â”‚
â”‚     Datos â”€â”¬â”€â–¶ Modelo 1 â”€â”€â”                                     â”‚
â”‚            â”œâ”€â–¶ Modelo 2 â”€â”€â”¼â”€â”€â–¶ VotaciÃ³n â”€â”€â–¶ PredicciÃ³n          â”‚
â”‚            â””â”€â–¶ Modelo 3 â”€â”€â”˜                                     â”‚
â”‚                                                                 â”‚
â”‚  2. BOOSTING                                                    â”‚
â”‚     â€¢ Entrena modelos en SECUENCIA                              â”‚
â”‚     â€¢ Cada modelo corrige errores del anterior                  â”‚
â”‚     â€¢ Reduce BIAS (underfitting)                                â”‚
â”‚     Ejemplos: AdaBoost, Gradient Boosting, XGBoost              â”‚
â”‚                                                                 â”‚
â”‚     Datos â”€â–¶ Modelo 1 â”€â–¶ Errores â”€â–¶ Modelo 2 â”€â–¶ Errores â”€â–¶ ...  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Random Forest

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CONCEPTO: RANDOM FOREST                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  "Bosque" de Ã¡rboles de decisiÃ³n que votan.                     â”‚
â”‚                                                                 â”‚
â”‚  DOS NIVELES DE ALEATORIEDAD:                                   â”‚
â”‚  1. Bootstrap: Cada Ã¡rbol entrena con muestra diferente         â”‚
â”‚  2. Random features: Cada divisiÃ³n considera subset de features â”‚
â”‚                                                                 â”‚
â”‚  POR QUÃ‰ FUNCIONA:                                              â”‚
â”‚  â€¢ Ãrboles individuales hacen overfitting (alta varianza)       â”‚
â”‚  â€¢ Pero sus errores son diferentes (no correlacionados)         â”‚
â”‚  â€¢ Al promediar, los errores se cancelan                        â”‚
â”‚                                                                 â”‚
â”‚  HIPERPARÃMETROS CLAVE:                                         â”‚
â”‚  â€¢ n_estimators: NÃºmero de Ã¡rboles (mÃ¡s = mejor, hasta un punto)â”‚
â”‚  â€¢ max_depth: Profundidad de Ã¡rboles                            â”‚
â”‚  â€¢ max_features: Features por divisiÃ³n ('sqrt' para clasif)     â”‚
â”‚  â€¢ min_samples_leaf: TamaÃ±o mÃ­nimo de hojas                     â”‚
â”‚                                                                 â”‚
â”‚  VENTAJAS:                                                      â”‚
â”‚  âœ… Muy buen rendimiento out-of-the-box                         â”‚
â”‚  âœ… DifÃ­cil de hacer overfitting                                â”‚
â”‚  âœ… Importancia de features gratis                              â”‚
â”‚  âœ… Funciona con pocos hiperparÃ¡metros                          â”‚
â”‚                                                                 â”‚
â”‚  LIMITACIONES:                                                  â”‚
â”‚  âŒ Menos interpretable que un Ã¡rbol                            â”‚
â”‚  âŒ Lento para predecir (muchos Ã¡rboles)                        â”‚
â”‚  âŒ No extrapola bien                                           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```python
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.datasets import make_classification

# Crear datos
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Entrenar Random Forest
rf = RandomForestClassifier(
    n_estimators=100,      # 100 Ã¡rboles
    max_depth=10,          # Limitar profundidad
    max_features='sqrt',   # âˆšn features por divisiÃ³n
    min_samples_leaf=5,
    n_jobs=-1,             # Usar todos los CPUs
    random_state=42
)
rf.fit(X_train, y_train)

print(f"Accuracy: {rf.score(X_test, y_test):.3f}")

# Importancia de features
importances = rf.feature_importances_
indices = np.argsort(importances)[::-1]

print("\nTop 5 features mÃ¡s importantes:")
for i in range(5):
    print(f"  Feature {indices[i]}: {importances[indices[i]]:.3f}")

# Visualizar importancias
plt.figure(figsize=(10, 6))
plt.bar(range(10), importances[indices[:10]])
plt.xticks(range(10), [f"F{indices[i]}" for i in range(10)])
plt.title("Importancia de Features")
plt.show()
```

### Gradient Boosting y XGBoost

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CONCEPTO: GRADIENT BOOSTING                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Construye Ã¡rboles SECUENCIALMENTE.                             â”‚
â”‚  Cada Ã¡rbol corrige los errores (residuos) del anterior.        â”‚
â”‚                                                                 â”‚
â”‚  PROCESO:                                                       â”‚
â”‚  1. Entrenar Ã¡rbol 1 â†’ predice yâ‚                               â”‚
â”‚  2. Calcular errores: râ‚ = y_real - yâ‚                          â”‚
â”‚  3. Entrenar Ã¡rbol 2 para predecir râ‚                           â”‚
â”‚  4. PredicciÃ³n total: yâ‚ + Ã¡rbol2(x)                            â”‚
â”‚  5. Repetir...                                                  â”‚
â”‚                                                                 â”‚
â”‚  XGBOOST (eXtreme Gradient Boosting):                           â”‚
â”‚  ImplementaciÃ³n optimizada con:                                 â”‚
â”‚  â€¢ RegularizaciÃ³n L1 y L2 incorporada                           â”‚
â”‚  â€¢ Manejo de valores faltantes                                  â”‚
â”‚  â€¢ ParalelizaciÃ³n eficiente                                     â”‚
â”‚  â€¢ Poda de Ã¡rboles inteligente                                  â”‚
â”‚                                                                 â”‚
â”‚  CUÃNDO USAR:                                                   â”‚
â”‚  âœ… Datos tabulares (el rey de Kaggle)                          â”‚
â”‚  âœ… Cuando necesitas mÃ¡ximo rendimiento                         â”‚
â”‚  âœ… Features mixtas (numÃ©ricas y categÃ³ricas)                   â”‚
â”‚                                                                 â”‚
â”‚  CUIDADO:                                                       â”‚
â”‚  âš ï¸ MÃ¡s propenso a overfitting que Random Forest                â”‚
â”‚  âš ï¸ MÃ¡s sensible a hiperparÃ¡metros                              â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```python
from sklearn.ensemble import GradientBoostingClassifier
# pip install xgboost
import xgboost as xgb

# Sklearn Gradient Boosting
gb = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.1,     # CuÃ¡nto contribuye cada Ã¡rbol
    max_depth=3,           # Ãrboles poco profundos
    random_state=42
)
gb.fit(X_train, y_train)
print(f"Gradient Boosting Accuracy: {gb.score(X_test, y_test):.3f}")

# XGBoost (mÃ¡s rÃ¡pido y potente)
xgb_model = xgb.XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    use_label_encoder=False,
    eval_metric='logloss',
    random_state=42
)
xgb_model.fit(X_train, y_train)
print(f"XGBoost Accuracy: {xgb_model.score(X_test, y_test):.3f}")

# Early stopping (detener cuando no mejora)
xgb_model_es = xgb.XGBClassifier(
    n_estimators=1000,  # Muchos, pero early stopping
    learning_rate=0.1,
    max_depth=3,
    early_stopping_rounds=10,
    random_state=42
)
xgb_model_es.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    verbose=False
)
print(f"XGBoost (early stop) - Mejores iteraciones: {xgb_model_es.best_iteration}")
```

---

## Support Vector Machines (SVM)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CONCEPTO: SUPPORT VECTOR MACHINES                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Encuentra el HIPERPLANO que mejor separa las clases            â”‚
â”‚  maximizando el MARGEN (distancia a los puntos mÃ¡s cercanos).   â”‚
â”‚                                                                 â”‚
â”‚  VISUALIZACIÃ“N 2D:                                              â”‚
â”‚                                                                 â”‚
â”‚       Clase A (â—)              Clase B (â—‹)                      â”‚
â”‚                                                                 â”‚
â”‚       â—  â—                        â—‹  â—‹                          â”‚
â”‚          â—      margen            â—‹                             â”‚
â”‚       â—     â†â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â†’   â—‹   â—‹                             â”‚
â”‚         â—      â”‚  SV   â”‚      â—‹                                 â”‚
â”‚       â—        â”‚â”€â”€â”€â”€â”€â”€â”€â”‚        â—‹  â—‹                            â”‚
â”‚                                                                 â”‚
â”‚  SV = Support Vectors (puntos en el margen, los que importan)   â”‚
â”‚                                                                 â”‚
â”‚  KERNEL TRICK (para datos no separables linealmente):           â”‚
â”‚  Proyecta datos a dimensiÃ³n superior donde SÃ son separables    â”‚
â”‚                                                                 â”‚
â”‚     2D:  No separable     3D: Separable con plano               â”‚
â”‚     â—‹ â— â— â—‹              â—‹         â—‹                            â”‚
â”‚     â— â—‹ â—‹ â—             â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€                           â”‚
â”‚                         â—‹           â—‹                           â”‚
â”‚                                                                 â”‚
â”‚  KERNELS COMUNES:                                               â”‚
â”‚  â€¢ 'linear': SeparaciÃ³n lineal                                  â”‚
â”‚  â€¢ 'rbf' (Radial Basis): El mÃ¡s usado, captura no linealidad    â”‚
â”‚  â€¢ 'poly': Polinomial                                           â”‚
â”‚                                                                 â”‚
â”‚  HIPERPARÃMETROS:                                               â”‚
â”‚  â€¢ C: PenalizaciÃ³n por errores (C alto = menos errores, overfÃ­t)â”‚
â”‚  â€¢ gamma: Radio de influencia de puntos (gamma alto = overfÃ­t)  â”‚
â”‚                                                                 â”‚
â”‚  VENTAJAS:                                                      â”‚
â”‚  âœ… Efectivo en alta dimensionalidad                            â”‚
â”‚  âœ… Funciona bien con margen claro                              â”‚
â”‚  âœ… Robusto a outliers (solo usa support vectors)               â”‚
â”‚                                                                 â”‚
â”‚  LIMITACIONES:                                                  â”‚
â”‚  âŒ Lento con muchos datos (O(nÂ²) o peor)                       â”‚
â”‚  âŒ Requiere escalado                                           â”‚
â”‚  âŒ No da probabilidades directamente                           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```python
from sklearn.svm import SVC, SVR
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

# SVM requiere escalado
svm_pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('svm', SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42))
])

svm_pipeline.fit(X_train, y_train)
print(f"SVM Accuracy: {svm_pipeline.score(X_test, y_test):.3f}")

# Comparar kernels
kernels = ['linear', 'rbf', 'poly']
for kernel in kernels:
    svm = Pipeline([
        ('scaler', StandardScaler()),
        ('svm', SVC(kernel=kernel, random_state=42))
    ])
    svm.fit(X_train, y_train)
    print(f"  {kernel}: {svm.score(X_test, y_test):.3f}")

# SVM con probabilidades
svm_prob = Pipeline([
    ('scaler', StandardScaler()),
    ('svm', SVC(kernel='rbf', probability=True, random_state=42))
])
svm_prob.fit(X_train, y_train)
probs = svm_prob.predict_proba(X_test[:5])
print(f"\nProbabilidades:\n{probs}")
```

---

## K-Nearest Neighbors (KNN)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CONCEPTO: K-NEAREST NEIGHBORS                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  "Dime con quiÃ©n andas y te dirÃ© quiÃ©n eres"                    â”‚
â”‚  Clasifica segÃºn los K vecinos mÃ¡s cercanos.                    â”‚
â”‚                                                                 â”‚
â”‚  PROCESO:                                                       â”‚
â”‚  1. Recibe punto nuevo (?)                                      â”‚
â”‚  2. Calcula distancia a todos los puntos de entrenamiento       â”‚
â”‚  3. Encuentra los K mÃ¡s cercanos                                â”‚
â”‚  4. Vota: la clase mayoritaria gana                             â”‚
â”‚                                                                 â”‚
â”‚  EJEMPLO con K=3:                                               â”‚
â”‚                                                                 â”‚
â”‚       â—       â—‹                                                 â”‚
â”‚         â— â—‹ [?]    â† Vecinos: 2 â—, 1 â—‹ â†’ Predice â—             â”‚
â”‚       â—   â—‹   â—‹                                                 â”‚
â”‚                                                                 â”‚
â”‚  DISTANCIAS:                                                    â”‚
â”‚  â€¢ Euclidiana: âˆš(Î£(xáµ¢ - yáµ¢)Â²)  (la mÃ¡s comÃºn)                   â”‚
â”‚  â€¢ Manhattan: Î£|xáµ¢ - yáµ¢|                                        â”‚
â”‚  â€¢ Minkowski: generalizaciÃ³n                                    â”‚
â”‚                                                                 â”‚
â”‚  K (nÃºmero de vecinos):                                         â”‚
â”‚  â€¢ K pequeÃ±o (1-3): Sensible a ruido, puede overfittear         â”‚
â”‚  â€¢ K grande: MÃ¡s suave, puede underfittear                      â”‚
â”‚  â€¢ Regla: K impar para evitar empates                           â”‚
â”‚                                                                 â”‚
â”‚  VENTAJAS:                                                      â”‚
â”‚  âœ… Simple e intuitivo                                          â”‚
â”‚  âœ… No hay entrenamiento (lazy learner)                         â”‚
â”‚  âœ… Se adapta naturalmente a nuevos datos                       â”‚
â”‚                                                                 â”‚
â”‚  LIMITACIONES:                                                  â”‚
â”‚  âŒ Lento en predicciÃ³n (calcula todas las distancias)          â”‚
â”‚  âŒ Sufre "maldiciÃ³n de dimensionalidad"                        â”‚
â”‚  âŒ Sensible a features irrelevantes                            â”‚
â”‚  âŒ REQUIERE ESCALADO (distancias)                              â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

# KNN REQUIERE escalado
knn_pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('knn', KNeighborsClassifier(n_neighbors=5))
])

knn_pipeline.fit(X_train, y_train)
print(f"KNN Accuracy: {knn_pipeline.score(X_test, y_test):.3f}")

# Encontrar mejor K
from sklearn.model_selection import cross_val_score

best_k = 1
best_score = 0

for k in range(1, 21, 2):  # Solo impares
    knn = Pipeline([
        ('scaler', StandardScaler()),
        ('knn', KNeighborsClassifier(n_neighbors=k))
    ])
    scores = cross_val_score(knn, X_train, y_train, cv=5)
    mean_score = scores.mean()
    
    if mean_score > best_score:
        best_score = mean_score
        best_k = k

print(f"\nMejor K: {best_k} (CV Score: {best_score:.3f})")
```

---

## ComparaciÃ³n de Algoritmos

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              GUÃA RÃPIDA DE SELECCIÃ“N                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  ESCENARIO                         â”‚  ALGORITMO SUGERIDO        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Baseline rÃ¡pido                   â”‚  Logistic/Linear Reg       â”‚
â”‚  Interpretabilidad mÃ¡xima          â”‚  Decision Tree, Logistic   â”‚
â”‚  MÃ¡ximo rendimiento (tabular)      â”‚  XGBoost, Random Forest    â”‚
â”‚  Datos pequeÃ±os                    â”‚  SVM, KNN                  â”‚
â”‚  Datos muy grandes                 â”‚  Random Forest, XGBoost    â”‚
â”‚  ClasificaciÃ³n de texto            â”‚  Naive Bayes, Logistic     â”‚
â”‚  Muchas features irrelevantes      â”‚  Lasso, Random Forest      â”‚
â”‚  Relaciones no lineales            â”‚  Random Forest, XGBoost    â”‚
â”‚                                                                 â”‚
â”‚  FLUJO DE TRABAJO TÃPICO:                                       â”‚
â”‚                                                                 â”‚
â”‚  1. Empezar con baseline simple (Logistic/Linear)               â”‚
â”‚  2. Probar Random Forest (robusto, pocos hiperparÃ¡metros)       â”‚
â”‚  3. Si necesitas mÃ¡s: XGBoost con tuning                        â”‚
â”‚  4. Si necesitas interpretabilidad: Tree o Logistic             â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```python
# ========================================
# COMPARACIÃ“N AUTOMÃTICA DE MODELOS
# ========================================
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

# Modelos a comparar
models = {
    'Logistic Regression': LogisticRegression(random_state=42),
    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42),
    'SVM': SVC(random_state=42),
    'KNN': KNeighborsClassifier(),
    'Naive Bayes': GaussianNB()
}

print("ComparaciÃ³n de Modelos (5-fold CV):\n")
print(f"{'Modelo':<25} {'Accuracy':<12} {'Std':<8}")
print("-" * 45)

results = []
for name, model in models.items():
    # SVM y KNN necesitan escalado
    if name in ['SVM', 'KNN']:
        model = Pipeline([
            ('scaler', StandardScaler()),
            ('model', model)
        ])
    
    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')
    results.append((name, scores.mean(), scores.std()))
    print(f"{name:<25} {scores.mean():.3f}        Â±{scores.std():.3f}")

# Mejor modelo
best = max(results, key=lambda x: x[1])
print(f"\nğŸ† Mejor modelo: {best[0]} ({best[1]:.3f})")
```

---

## ğŸ·ï¸ Tags

#programming #machinelearning #python #algorithms #datascience

---

## ğŸ“š Ver TambiÃ©n

- [[ML Fundamentals Guide|Fundamentos de ML]]
- [[Deep Learning Guide|Deep Learning - GuÃ­a Completa]]
- [[ML Projects Guide|Proyectos PrÃ¡cticos]]
