# Machine Learning - Proyectos PrÃ¡cticos

> **Proyectos completos paso a paso para practicar ML**
> Ãšltima actualizaciÃ³n: Enero 2026

---

## Tabla de Contenidos

1. [[#Proyecto 1 - PredicciÃ³n de Precios de Casas|RegresiÃ³n: Precios de Casas]]
2. [[#Proyecto 2 - ClasificaciÃ³n de Sentimientos|NLP: AnÃ¡lisis de Sentimientos]]
3. [[#Proyecto 3 - ClasificaciÃ³n de ImÃ¡genes|CNN: Clasificador de ImÃ¡genes]]
4. [[#Proyecto 4 - Sistema de RecomendaciÃ³n|RecSys: Recomendaciones]]
5. [[#Herramientas y Setup|Setup del Entorno]]

---

## Herramientas y Setup

### InstalaciÃ³n del Entorno

```bash
# Crear entorno virtual
python -m venv ml_env
source ml_env/bin/activate  # Linux/Mac
ml_env\Scripts\activate     # Windows

# LibrerÃ­as fundamentales
pip install numpy pandas matplotlib seaborn scikit-learn

# Deep Learning
pip install torch torchvision torchaudio
pip install transformers datasets

# Extras Ãºtiles
pip install jupyter notebook
pip install xgboost lightgbm
pip install plotly
```

### LibrerÃ­as Principales

```python
# ========================================
# IMPORTS ESTÃNDAR PARA ML
# ========================================

# ManipulaciÃ³n de datos
import numpy as np
import pandas as pd

# VisualizaciÃ³n
import matplotlib.pyplot as plt
import seaborn as sns
plt.style.use('seaborn-v0_8-whitegrid')

# Sklearn - Preprocesamiento
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.impute import SimpleImputer

# Sklearn - Modelos
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.ensemble import GradientBoostingClassifier

# Sklearn - MÃ©tricas
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    mean_squared_error, r2_score, classification_report,
    confusion_matrix, roc_auc_score, roc_curve
)

# Sklearn - Pipelines
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

# ConfiguraciÃ³n
import warnings
warnings.filterwarnings('ignore')

# Reproducibilidad
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)
```

---

## Proyecto 1 - PredicciÃ³n de Precios de Casas

### Objetivo
Predecir el precio de venta de casas basÃ¡ndose en sus caracterÃ­sticas.

```python
# ========================================
# PROYECTO: PREDICCIÃ“N DE PRECIOS
# ========================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# ----------------------------------------
# 1. CARGAR Y EXPLORAR DATOS
# ----------------------------------------

# Crear dataset sintÃ©tico (en la prÃ¡ctica usarÃ­as datos reales)
np.random.seed(42)
n_samples = 1000

data = pd.DataFrame({
    'area_m2': np.random.uniform(50, 300, n_samples),
    'habitaciones': np.random.randint(1, 6, n_samples),
    'baÃ±os': np.random.randint(1, 4, n_samples),
    'antiguedad': np.random.randint(0, 50, n_samples),
    'tiene_garage': np.random.choice([0, 1], n_samples),
    'tiene_jardin': np.random.choice([0, 1], n_samples),
    'piso': np.random.randint(0, 20, n_samples),
    'distancia_centro_km': np.random.uniform(0.5, 30, n_samples),
    'zona': np.random.choice(['norte', 'sur', 'este', 'oeste'], n_samples)
})

# Crear precio (relaciÃ³n realista)
data['precio'] = (
    data['area_m2'] * 2000 +
    data['habitaciones'] * 15000 +
    data['baÃ±os'] * 10000 -
    data['antiguedad'] * 500 +
    data['tiene_garage'] * 20000 +
    data['tiene_jardin'] * 15000 -
    data['distancia_centro_km'] * 1500 +
    np.random.normal(0, 20000, n_samples)
)

print("="*50)
print("EXPLORACIÃ“N DE DATOS")
print("="*50)
print(f"\nShape: {data.shape}")
print(f"\nPrimeras filas:\n{data.head()}")
print(f"\nEstadÃ­sticas:\n{data.describe()}")
print(f"\nValores nulos:\n{data.isnull().sum()}")

# ----------------------------------------
# 2. ANÃLISIS EXPLORATORIO (EDA)
# ----------------------------------------

fig, axes = plt.subplots(2, 3, figsize=(15, 10))

# DistribuciÃ³n del precio
axes[0, 0].hist(data['precio'], bins=30, edgecolor='black')
axes[0, 0].set_title('DistribuciÃ³n de Precios')
axes[0, 0].set_xlabel('Precio')

# Precio vs Area
axes[0, 1].scatter(data['area_m2'], data['precio'], alpha=0.5)
axes[0, 1].set_title('Precio vs Ãrea')
axes[0, 1].set_xlabel('Ãrea (mÂ²)')
axes[0, 1].set_ylabel('Precio')

# Precio vs Habitaciones
data.boxplot(column='precio', by='habitaciones', ax=axes[0, 2])
axes[0, 2].set_title('Precio por Habitaciones')

# Matriz de correlaciÃ³n
numeric_cols = data.select_dtypes(include=[np.number]).columns
corr_matrix = data[numeric_cols].corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', ax=axes[1, 0], fmt='.2f')
axes[1, 0].set_title('CorrelaciÃ³n')

# Precio por zona
data.groupby('zona')['precio'].mean().plot(kind='bar', ax=axes[1, 1])
axes[1, 1].set_title('Precio Promedio por Zona')
axes[1, 1].tick_params(axis='x', rotation=45)

# Precio vs AntigÃ¼edad
axes[1, 2].scatter(data['antiguedad'], data['precio'], alpha=0.5)
axes[1, 2].set_title('Precio vs AntigÃ¼edad')
axes[1, 2].set_xlabel('AntigÃ¼edad (aÃ±os)')

plt.tight_layout()
plt.show()

# ----------------------------------------
# 3. PREPARACIÃ“N DE DATOS
# ----------------------------------------

# Separar features y target
X = data.drop('precio', axis=1)
y = data['precio']

# One-hot encoding para zona
X = pd.get_dummies(X, columns=['zona'], prefix='zona')

# DivisiÃ³n train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Escalar features numÃ©ricas
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("\n" + "="*50)
print("DATOS PREPARADOS")
print("="*50)
print(f"Train: {X_train.shape}, Test: {X_test.shape}")
print(f"Features: {list(X.columns)}")

# ----------------------------------------
# 4. ENTRENAR Y COMPARAR MODELOS
# ----------------------------------------

models = {
    'Linear Regression': LinearRegression(),
    'Ridge': Ridge(alpha=1.0),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)
}

print("\n" + "="*50)
print("COMPARACIÃ“N DE MODELOS")
print("="*50)

results = []
for name, model in models.items():
    # Cross-validation
    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2')
    
    # Entrenar y predecir
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    
    # MÃ©tricas
    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae = mean_absolute_error(y_test, y_pred)
    
    results.append({
        'Modelo': name,
        'RÂ² CV': f"{cv_scores.mean():.3f} Â± {cv_scores.std():.3f}",
        'RÂ² Test': f"{r2:.3f}",
        'RMSE': f"{rmse:,.0f}",
        'MAE': f"{mae:,.0f}"
    })
    
results_df = pd.DataFrame(results)
print(results_df.to_string(index=False))

# ----------------------------------------
# 5. ANALIZAR MEJOR MODELO
# ----------------------------------------

best_model = models['Random Forest']
best_model.fit(X_train_scaled, y_train)
y_pred = best_model.predict(X_test_scaled)

# Importancia de features
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': best_model.feature_importances_
}).sort_values('importance', ascending=False)

print("\n" + "="*50)
print("IMPORTANCIA DE FEATURES (Random Forest)")
print("="*50)
print(feature_importance.to_string(index=False))

# Visualizar predicciones vs reales
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Predicciones vs Reales
axes[0].scatter(y_test, y_pred, alpha=0.5)
axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
axes[0].set_xlabel('Precio Real')
axes[0].set_ylabel('Precio Predicho')
axes[0].set_title('Predicciones vs Valores Reales')

# Residuos
residuals = y_test - y_pred
axes[1].hist(residuals, bins=30, edgecolor='black')
axes[1].axvline(x=0, color='r', linestyle='--')
axes[1].set_xlabel('Residuo')
axes[1].set_title('DistribuciÃ³n de Residuos')

plt.tight_layout()
plt.show()

# ----------------------------------------
# 6. PREDICCIÃ“N CON NUEVA CASA
# ----------------------------------------

print("\n" + "="*50)
print("PREDICCIÃ“N DE NUEVA CASA")
print("="*50)

nueva_casa = pd.DataFrame({
    'area_m2': [150],
    'habitaciones': [3],
    'baÃ±os': [2],
    'antiguedad': [5],
    'tiene_garage': [1],
    'tiene_jardin': [1],
    'piso': [3],
    'distancia_centro_km': [8],
    'zona_este': [0],
    'zona_norte': [1],
    'zona_oeste': [0],
    'zona_sur': [0]
})

nueva_casa_scaled = scaler.transform(nueva_casa)
precio_predicho = best_model.predict(nueva_casa_scaled)[0]

print(f"\nCaracterÃ­sticas:")
print(f"  - Ãrea: 150 mÂ²")
print(f"  - Habitaciones: 3")
print(f"  - BaÃ±os: 2")
print(f"  - AntigÃ¼edad: 5 aÃ±os")
print(f"  - Con garage y jardÃ­n")
print(f"  - Zona norte, 8km del centro")
print(f"\nğŸ’° Precio Predicho: ${precio_predicho:,.0f}")
```

---

## Proyecto 2 - ClasificaciÃ³n de Sentimientos

### Objetivo
Clasificar reseÃ±as de pelÃ­culas como positivas o negativas.

```python
# ========================================
# PROYECTO: ANÃLISIS DE SENTIMIENTOS
# ========================================
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import re

# ----------------------------------------
# 1. CREAR/CARGAR DATOS
# ----------------------------------------

# Dataset de ejemplo (en prÃ¡ctica: IMDB, Yelp, Twitter, etc.)
reviews = [
    # Positivas
    "Esta pelÃ­cula es increÃ­ble, la mejor que he visto",
    "Excelente actuaciÃ³n y una historia conmovedora",
    "Me encantÃ³ cada minuto, muy recomendada",
    "Una obra maestra del cine contemporÃ¡neo",
    "Los efectos especiales son impresionantes",
    "La mejor pelÃ­cula del aÃ±o sin duda",
    "Historia emotiva con un final perfecto",
    "Gran direcciÃ³n y fotografÃ­a espectacular",
    "Actuaciones brillantes de todo el elenco",
    "Me hizo reÃ­r y llorar, pelÃ­cula completa",
    "IncreÃ­ble desarrollo de personajes",
    "Visualmente impresionante y bien escrita",
    
    # Negativas
    "PelÃ­cula aburrida y predecible",
    "No vale la pena perder el tiempo",
    "PÃ©simas actuaciones y guiÃ³n terrible",
    "La peor pelÃ­cula que he visto en aÃ±os",
    "Historia sin sentido y mal ejecutada",
    "Totalmente decepcionante y aburrida",
    "No la recomiendo para nada",
    "Desperdicio de tiempo y dinero",
    "Efectos baratos y actuaciÃ³n horrible",
    "Final absurdo que arruina todo",
    "PelÃ­cula vacÃ­a y sin emociÃ³n",
    "Muy mala, evÃ­tenla a toda costa"
]

labels = [1]*12 + [0]*12  # 1=positivo, 0=negativo

# Convertir a DataFrame
data = pd.DataFrame({'text': reviews, 'sentiment': labels})
print("Dataset:")
print(data.head(10))
print(f"\nDistribuciÃ³n: {data['sentiment'].value_counts().to_dict()}")

# ----------------------------------------
# 2. PREPROCESAMIENTO DE TEXTO
# ----------------------------------------

def preprocess_text(text):
    """Limpieza bÃ¡sica de texto"""
    # MinÃºsculas
    text = text.lower()
    # Eliminar caracteres especiales
    text = re.sub(r'[^\w\s]', '', text)
    # Eliminar nÃºmeros
    text = re.sub(r'\d+', '', text)
    # Eliminar espacios extra
    text = ' '.join(text.split())
    return text

data['text_clean'] = data['text'].apply(preprocess_text)
print("\nTexto limpio:")
print(data[['text', 'text_clean']].head())

# ----------------------------------------
# 3. VECTORIZACIÃ“N (TF-IDF)
# ----------------------------------------

# DivisiÃ³n
X_train, X_test, y_train, y_test = train_test_split(
    data['text_clean'], data['sentiment'], 
    test_size=0.25, random_state=42
)

# TF-IDF Vectorizer
vectorizer = TfidfVectorizer(
    max_features=1000,      # MÃ¡ximo vocabulario
    ngram_range=(1, 2),     # Unigramas y bigramas
    min_df=1,               # MÃ­nimo documentos
    stop_words=None         # Podemos aÃ±adir stopwords en espaÃ±ol
)

X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

print(f"\nVocabulario: {len(vectorizer.vocabulary_)} tÃ©rminos")
print(f"Shape train: {X_train_tfidf.shape}")

# Top tÃ©rminos por TF-IDF
feature_names = vectorizer.get_feature_names_out()
print(f"\nAlgunos tÃ©rminos: {list(feature_names[:20])}")

# ----------------------------------------
# 4. ENTRENAR MODELOS
# ----------------------------------------

models = {
    'Logistic Regression': LogisticRegression(random_state=42),
    'Naive Bayes': MultinomialNB(),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)
}

print("\n" + "="*50)
print("RESULTADOS")
print("="*50)

for name, model in models.items():
    model.fit(X_train_tfidf, y_train)
    y_pred = model.predict(X_test_tfidf)
    
    print(f"\n{name}:")
    print(classification_report(y_test, y_pred, 
                                target_names=['Negativo', 'Positivo']))

# ----------------------------------------
# 5. ANALIZAR MODELO
# ----------------------------------------

# Usar Logistic Regression para interpretabilidad
best_model = models['Logistic Regression']

# Palabras mÃ¡s influyentes
coef = best_model.coef_[0]
top_positive = np.argsort(coef)[-10:]
top_negative = np.argsort(coef)[:10]

print("\n" + "="*50)
print("PALABRAS MÃS INFLUYENTES")
print("="*50)
print("\nğŸŸ¢ Palabras positivas:")
for idx in top_positive[::-1]:
    print(f"  {feature_names[idx]}: {coef[idx]:.3f}")

print("\nğŸ”´ Palabras negativas:")
for idx in top_negative:
    print(f"  {feature_names[idx]}: {coef[idx]:.3f}")

# ----------------------------------------
# 6. PREDECIR NUEVAS RESEÃ‘AS
# ----------------------------------------

print("\n" + "="*50)
print("PREDICCIONES NUEVAS")
print("="*50)

new_reviews = [
    "Una pelÃ­cula fantÃ¡stica que disfrutÃ© mucho",
    "No me gustÃ³ nada, muy aburrida",
    "Regular, tiene cosas buenas y malas"
]

for review in new_reviews:
    review_clean = preprocess_text(review)
    review_tfidf = vectorizer.transform([review_clean])
    
    prediction = best_model.predict(review_tfidf)[0]
    probability = best_model.predict_proba(review_tfidf)[0]
    
    sentiment = "ğŸ˜Š Positivo" if prediction == 1 else "ğŸ˜ Negativo"
    confidence = max(probability) * 100
    
    print(f"\n\"{review}\"")
    print(f"  â†’ {sentiment} (Confianza: {confidence:.1f}%)")
```

---

## Proyecto 3 - ClasificaciÃ³n de ImÃ¡genes

### Objetivo
Clasificar imÃ¡genes usando CNN con PyTorch.

```python
# ========================================
# PROYECTO: CLASIFICACIÃ“N DE IMÃGENES (CNN)
# ========================================
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np

# ConfiguraciÃ³n
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Usando: {device}")

# ----------------------------------------
# 1. CARGAR DATOS (CIFAR-10)
# ----------------------------------------

# Data augmentation para entrenamiento
train_transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.RandomCrop(32, padding=4),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), 
                         (0.2023, 0.1994, 0.2010))
])

test_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), 
                         (0.2023, 0.1994, 0.2010))
])

# Descargar CIFAR-10
train_dataset = torchvision.datasets.CIFAR10(
    root='./data', train=True, download=True, transform=train_transform
)
test_dataset = torchvision.datasets.CIFAR10(
    root='./data', train=False, download=True, transform=test_transform
)

# DataLoaders
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)

classes = ('aviÃ³n', 'auto', 'pÃ¡jaro', 'gato', 'venado', 
           'perro', 'rana', 'caballo', 'barco', 'camiÃ³n')

print(f"Train: {len(train_dataset)}, Test: {len(test_dataset)}")
print(f"Clases: {classes}")

# Visualizar algunas imÃ¡genes
def show_images(loader, classes, n=8):
    images, labels = next(iter(loader))
    fig, axes = plt.subplots(1, n, figsize=(15, 2))
    for i in range(n):
        img = images[i].permute(1, 2, 0).numpy()
        img = img * np.array([0.2023, 0.1994, 0.2010]) + np.array([0.4914, 0.4822, 0.4465])
        img = np.clip(img, 0, 1)
        axes[i].imshow(img)
        axes[i].set_title(classes[labels[i]])
        axes[i].axis('off')
    plt.show()

show_images(train_loader, classes)

# ----------------------------------------
# 2. DEFINIR CNN
# ----------------------------------------

class CNN(nn.Module):
    def __init__(self, num_classes=10):
        super(CNN, self).__init__()
        
        self.features = nn.Sequential(
            # Bloque 1
            nn.Conv2d(3, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.Conv2d(32, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            nn.Dropout2d(0.25),
            
            # Bloque 2
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            nn.Dropout2d(0.25),
            
            # Bloque 3
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.Conv2d(128, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            nn.Dropout2d(0.25),
        )
        
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(128 * 4 * 4, 512),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, num_classes)
        )
    
    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x

model = CNN(num_classes=10).to(device)
print(model)

# Contar parÃ¡metros
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"\nParÃ¡metros totales: {total_params:,}")
print(f"ParÃ¡metros entrenables: {trainable_params:,}")

# ----------------------------------------
# 3. ENTRENAR
# ----------------------------------------

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)

def train_epoch(model, loader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    
    for images, labels in loader:
        images, labels = images.to(device), labels.to(device)
        
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()
    
    return running_loss / len(loader), 100. * correct / total

def evaluate(model, loader, criterion, device):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for images, labels in loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
    
    return running_loss / len(loader), 100. * correct / total

# Entrenamiento
epochs = 20
history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}

print("\nEntrenando...")
for epoch in range(epochs):
    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)
    val_loss, val_acc = evaluate(model, test_loader, criterion, device)
    
    scheduler.step(val_loss)
    
    history['train_loss'].append(train_loss)
    history['train_acc'].append(train_acc)
    history['val_loss'].append(val_loss)
    history['val_acc'].append(val_acc)
    
    print(f"Epoch {epoch+1}/{epochs}")
    print(f"  Train Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%")
    print(f"  Val Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%")

# ----------------------------------------
# 4. VISUALIZAR RESULTADOS
# ----------------------------------------

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Loss
axes[0].plot(history['train_loss'], label='Train')
axes[0].plot(history['val_loss'], label='Validation')
axes[0].set_xlabel('Epoch')
axes[0].set_ylabel('Loss')
axes[0].set_title('Loss durante entrenamiento')
axes[0].legend()

# Accuracy
axes[1].plot(history['train_acc'], label='Train')
axes[1].plot(history['val_acc'], label='Validation')
axes[1].set_xlabel('Epoch')
axes[1].set_ylabel('Accuracy (%)')
axes[1].set_title('Accuracy durante entrenamiento')
axes[1].legend()

plt.tight_layout()
plt.show()

# ----------------------------------------
# 5. MATRIZ DE CONFUSIÃ“N
# ----------------------------------------

from sklearn.metrics import confusion_matrix
import seaborn as sns

model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        outputs = model(images)
        _, preds = outputs.max(1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.numpy())

cm = confusion_matrix(all_labels, all_preds)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=classes, yticklabels=classes)
plt.xlabel('Predicho')
plt.ylabel('Real')
plt.title('Matriz de ConfusiÃ³n')
plt.show()

# Accuracy por clase
print("\nAccuracy por clase:")
for i, cls in enumerate(classes):
    class_correct = cm[i, i]
    class_total = cm[i].sum()
    print(f"  {cls}: {100 * class_correct / class_total:.1f}%")
```

---

## Proyecto 4 - Sistema de RecomendaciÃ³n

### Objetivo
Crear un sistema de recomendaciÃ³n de pelÃ­culas.

```python
# ========================================
# PROYECTO: SISTEMA DE RECOMENDACIÃ“N
# ========================================
import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer

# ----------------------------------------
# 1. CREAR DATOS DE EJEMPLO
# ----------------------------------------

# PelÃ­culas con metadata
movies = pd.DataFrame({
    'movie_id': range(1, 11),
    'title': [
        'The Matrix', 'Inception', 'Interstellar', 
        'The Dark Knight', 'Pulp Fiction',
        'Forrest Gump', 'The Shawshank Redemption', 
        'Fight Club', 'The Godfather', 'Goodfellas'
    ],
    'genre': [
        'sci-fi action', 'sci-fi thriller', 'sci-fi drama',
        'action crime', 'crime drama',
        'drama romance', 'drama', 
        'drama thriller', 'crime drama', 'crime drama'
    ],
    'director': [
        'Wachowski', 'Nolan', 'Nolan',
        'Nolan', 'Tarantino',
        'Zemeckis', 'Darabont',
        'Fincher', 'Coppola', 'Scorsese'
    ],
    'year': [1999, 2010, 2014, 2008, 1994, 1994, 1994, 1999, 1972, 1990]
})

# Ratings de usuarios
np.random.seed(42)
n_users = 20
n_movies = 10

# Crear matriz de ratings (algunos NaN)
ratings_matrix = np.random.choice([np.nan, 1, 2, 3, 4, 5], 
                                   size=(n_users, n_movies),
                                   p=[0.5, 0.1, 0.1, 0.1, 0.1, 0.1])
ratings_df = pd.DataFrame(ratings_matrix, 
                          columns=movies['title'],
                          index=[f'User_{i}' for i in range(n_users)])

print("PelÃ­culas:")
print(movies)
print("\nMatriz de Ratings (muestra):")
print(ratings_df.head())

# ----------------------------------------
# 2. RECOMENDACIÃ“N BASADA EN CONTENIDO
# ----------------------------------------

print("\n" + "="*50)
print("RECOMENDACIÃ“N BASADA EN CONTENIDO")
print("="*50)

# Combinar features de contenido
movies['content'] = movies['genre'] + ' ' + movies['director']

# Vectorizar contenido
tfidf = TfidfVectorizer(stop_words='english')
content_matrix = tfidf.fit_transform(movies['content'])

# Similitud entre pelÃ­culas
content_similarity = cosine_similarity(content_matrix)
content_sim_df = pd.DataFrame(content_similarity,
                              index=movies['title'],
                              columns=movies['title'])

def get_content_recommendations(movie_title, n=5):
    """Recomienda pelÃ­culas similares basado en contenido"""
    if movie_title not in content_sim_df.columns:
        return f"PelÃ­cula '{movie_title}' no encontrada"
    
    # Obtener similitudes
    similarities = content_sim_df[movie_title]
    
    # Ordenar y excluir la pelÃ­cula misma
    similar_movies = similarities.sort_values(ascending=False)[1:n+1]
    
    return similar_movies

# Ejemplo
print("\nPelÃ­culas similares a 'Inception':")
recs = get_content_recommendations('Inception')
for title, score in recs.items():
    print(f"  {title}: {score:.3f}")

print("\nPelÃ­culas similares a 'The Godfather':")
recs = get_content_recommendations('The Godfather')
for title, score in recs.items():
    print(f"  {title}: {score:.3f}")

# ----------------------------------------
# 3. FILTRADO COLABORATIVO
# ----------------------------------------

print("\n" + "="*50)
print("FILTRADO COLABORATIVO")
print("="*50)

# Rellenar NaN con promedio del usuario para calcular similitud
ratings_filled = ratings_df.apply(lambda x: x.fillna(x.mean()), axis=1)

# Similitud entre usuarios
user_similarity = cosine_similarity(ratings_filled)
user_sim_df = pd.DataFrame(user_similarity,
                           index=ratings_df.index,
                           columns=ratings_df.index)

def get_user_recommendations(user_id, n=5):
    """Recomienda pelÃ­culas basado en usuarios similares"""
    # PelÃ­culas no vistas por el usuario
    user_ratings = ratings_df.loc[user_id]
    unwatched = user_ratings[user_ratings.isna()].index.tolist()
    
    if not unwatched:
        return "Usuario ha visto todas las pelÃ­culas"
    
    # Encontrar usuarios similares
    similar_users = user_sim_df[user_id].sort_values(ascending=False)[1:6]
    
    # Predecir ratings para pelÃ­culas no vistas
    predictions = {}
    for movie in unwatched:
        movie_ratings = []
        weights = []
        
        for similar_user, similarity in similar_users.items():
            rating = ratings_df.loc[similar_user, movie]
            if not np.isnan(rating):
                movie_ratings.append(rating)
                weights.append(similarity)
        
        if movie_ratings:
            # Weighted average
            predictions[movie] = np.average(movie_ratings, weights=weights)
    
    # Ordenar por predicciÃ³n
    sorted_predictions = sorted(predictions.items(), key=lambda x: x[1], reverse=True)
    return sorted_predictions[:n]

# Ejemplo
print(f"\nRecomendaciones para User_0:")
recs = get_user_recommendations('User_0')
for movie, predicted_rating in recs:
    print(f"  {movie}: {predicted_rating:.2f} â­")

# ----------------------------------------
# 4. SISTEMA HÃBRIDO
# ----------------------------------------

print("\n" + "="*50)
print("SISTEMA HÃBRIDO")
print("="*50)

def hybrid_recommendations(user_id, n=5, content_weight=0.3):
    """Combina filtrado colaborativo y basado en contenido"""
    
    # Obtener recomendaciones colaborativas
    collab_recs = get_user_recommendations(user_id, n=10)
    if isinstance(collab_recs, str):
        return collab_recs
    
    collab_dict = {movie: score for movie, score in collab_recs}
    
    # Para cada pelÃ­cula, aÃ±adir bonus por similitud de contenido
    # con pelÃ­culas que el usuario ha calificado alto
    user_ratings = ratings_df.loc[user_id].dropna()
    liked_movies = user_ratings[user_ratings >= 4].index.tolist()
    
    final_scores = {}
    for movie, collab_score in collab_dict.items():
        # Score de contenido: similitud promedio con pelÃ­culas gustadas
        content_scores = []
        for liked in liked_movies:
            if movie in content_sim_df.columns and liked in content_sim_df.columns:
                content_scores.append(content_sim_df.loc[movie, liked])
        
        content_score = np.mean(content_scores) if content_scores else 0
        
        # Combinar
        final_scores[movie] = (1 - content_weight) * collab_score + content_weight * (content_score * 5)
    
    # Ordenar
    sorted_recs = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)
    return sorted_recs[:n]

# Ejemplo
print(f"\nRecomendaciones hÃ­bridas para User_0:")
recs = hybrid_recommendations('User_0')
for movie, score in recs:
    print(f"  {movie}: {score:.2f}")

# ----------------------------------------
# 5. EVALUACIÃ“N
# ----------------------------------------

print("\n" + "="*50)
print("EVALUACIÃ“N DEL SISTEMA")
print("="*50)

def evaluate_recommendations(test_ratings, predictions):
    """Calcula RMSE entre predicciones y ratings reales"""
    errors = []
    for (user, movie), true_rating in test_ratings.items():
        if (user, movie) in predictions:
            pred_rating = predictions[(user, movie)]
            errors.append((true_rating - pred_rating) ** 2)
    
    if errors:
        rmse = np.sqrt(np.mean(errors))
        return rmse
    return None

# Simular evaluaciÃ³n (en prÃ¡ctica: dividir datos en train/test)
print("\nMÃ©tricas de evaluaciÃ³n:")
print("  - Coverage: % de items que el sistema puede recomendar")
print("  - Diversity: Variedad en las recomendaciones")
print("  - Novelty: QuÃ© tan 'sorprendentes' son las recomendaciones")
print("  - RMSE: Error en predicciÃ³n de ratings")
```

---

## Checklist de Proyecto ML

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CHECKLIST PARA PROYECTOS DE ML                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  FASE 1: DEFINICIÃ“N                                             â”‚
â”‚  â–¡ Definir objetivo de negocio claramente                       â”‚
â”‚  â–¡ Definir mÃ©trica de Ã©xito                                     â”‚
â”‚  â–¡ Establecer baseline (modelo simple o regla)                  â”‚
â”‚                                                                 â”‚
â”‚  FASE 2: DATOS                                                  â”‚
â”‚  â–¡ Recolectar/obtener datos                                     â”‚
â”‚  â–¡ EDA (Exploratory Data Analysis)                              â”‚
â”‚  â–¡ Verificar calidad de datos                                   â”‚
â”‚  â–¡ Manejar valores faltantes                                    â”‚
â”‚  â–¡ Manejar outliers                                             â”‚
â”‚  â–¡ Feature engineering                                          â”‚
â”‚  â–¡ Dividir train/val/test                                       â”‚
â”‚                                                                 â”‚
â”‚  FASE 3: MODELADO                                               â”‚
â”‚  â–¡ Empezar con modelo simple (baseline)                         â”‚
â”‚  â–¡ Probar varios algoritmos                                     â”‚
â”‚  â–¡ Cross-validation                                             â”‚
â”‚  â–¡ Hyperparameter tuning                                        â”‚
â”‚  â–¡ Analizar errores                                             â”‚
â”‚                                                                 â”‚
â”‚  FASE 4: EVALUACIÃ“N                                             â”‚
â”‚  â–¡ Evaluar en test set (UNA vez)                                â”‚
â”‚  â–¡ Comparar con baseline                                        â”‚
â”‚  â–¡ Verificar no hay data leakage                                â”‚
â”‚  â–¡ Interpretar resultados                                       â”‚
â”‚                                                                 â”‚
â”‚  FASE 5: DESPLIEGUE                                             â”‚
â”‚  â–¡ Guardar modelo (pickle, joblib, torch.save)                  â”‚
â”‚  â–¡ Crear pipeline de preprocesamiento                           â”‚
â”‚  â–¡ API o integraciÃ³n                                            â”‚
â”‚  â–¡ Monitoreo en producciÃ³n                                      â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ·ï¸ Tags

#programming #machinelearning #python #projects #datascience

---

## ğŸ“š Ver TambiÃ©n

- [[ML Fundamentals Guide|Fundamentos de ML]]
- [[ML Algorithms Guide|Algoritmos de ML]]
- [[Deep Learning Guide|Deep Learning]]
